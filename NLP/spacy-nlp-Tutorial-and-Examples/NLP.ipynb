
!conda create -n spacy_env python=3.11 -y
Channels:
 - defaults
Platform: win-64
Collecting package metadata (repodata.json): ...working... done
Solving environment: ...working... done

## Package Plan ##

  environment location: C:\Users\444\anaconda3\envs\spacy_env

  added / updated specs:
    - python=3.11


The following NEW packages will be INSTALLED:

  bzip2              pkgs/main/win-64::bzip2-1.0.8-h2bbff1b_6 
  ca-certificates    pkgs/main/win-64::ca-certificates-2025.2.25-haa95532_0 
  libffi             pkgs/main/win-64::libffi-3.4.4-hd77b12b_1 
  openssl            pkgs/main/win-64::openssl-3.0.16-h3f729d1_0 
  pip                pkgs/main/noarch::pip-25.1-pyhc872135_2 
  python             pkgs/main/win-64::python-3.11.11-h4607a30_0 
  setuptools         pkgs/main/win-64::setuptools-78.1.1-py311haa95532_0 
  sqlite             pkgs/main/win-64::sqlite-3.45.3-h2bbff1b_0 
  tk                 pkgs/main/win-64::tk-8.6.14-h5e9d12e_1 
  tzdata             pkgs/main/noarch::tzdata-2025b-h04d1e81_0 
  vc                 pkgs/main/win-64::vc-14.42-haa95532_5 
  vs2015_runtime     pkgs/main/win-64::vs2015_runtime-14.42.34433-hbfb602d_5 
  wheel              pkgs/main/win-64::wheel-0.45.1-py311haa95532_0 
  xz                 pkgs/main/win-64::xz-5.6.4-h4754444_1 
  zlib               pkgs/main/win-64::zlib-1.2.13-h8cc25b3_1 



Downloading and Extracting Packages: ...working... done
Preparing transaction: ...working... done
Verifying transaction: ...working... done
Executing transaction: ...working... done
#
# To activate this environment, use
#
#     $ conda activate spacy_env
#
# To deactivate an active environment, use
#
#     $ conda deactivate

!python -m ipykernel install --user --name spacy_env --display-name "Python (spacy_env)"
Installed kernelspec spacy_env in C:\Users\444\AppData\Roaming\jupyter\kernels\spacy_env
0.00s - Debugger warning: It seems that frozen modules are being used, which may
0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off
0.00s - to python to disable frozen modules.
0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.
!pip install spacy
Requirement already satisfied: spacy in c:\users\444\anaconda3\lib\site-packages (3.8.7)
Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\users\444\anaconda3\lib\site-packages (from spacy) (3.0.12)
Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\users\444\anaconda3\lib\site-packages (from spacy) (1.0.5)
Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\users\444\anaconda3\lib\site-packages (from spacy) (1.0.13)
Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\users\444\anaconda3\lib\site-packages (from spacy) (2.0.11)
Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\users\444\anaconda3\lib\site-packages (from spacy) (3.0.10)
Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\users\444\anaconda3\lib\site-packages (from spacy) (8.3.6)
Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\users\444\anaconda3\lib\site-packages (from spacy) (1.1.3)
Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\users\444\anaconda3\lib\site-packages (from spacy) (2.5.1)
Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\users\444\anaconda3\lib\site-packages (from spacy) (2.0.10)
Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\users\444\anaconda3\lib\site-packages (from spacy) (0.4.1)
Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\users\444\anaconda3\lib\site-packages (from spacy) (0.16.0)
Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\users\444\anaconda3\lib\site-packages (from spacy) (4.66.5)
Requirement already satisfied: numpy>=1.19.0 in c:\users\444\anaconda3\lib\site-packages (from spacy) (2.2.6)
Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\users\444\anaconda3\lib\site-packages (from spacy) (2.32.3)
Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\users\444\anaconda3\lib\site-packages (from spacy) (2.8.2)
Requirement already satisfied: jinja2 in c:\users\444\anaconda3\lib\site-packages (from spacy) (3.1.4)
Requirement already satisfied: setuptools in c:\users\444\anaconda3\lib\site-packages (from spacy) (75.1.0)
Requirement already satisfied: packaging>=20.0 in c:\users\444\anaconda3\lib\site-packages (from spacy) (24.1)
Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\users\444\anaconda3\lib\site-packages (from spacy) (3.5.0)
Requirement already satisfied: language-data>=1.2 in c:\users\444\anaconda3\lib\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)
Requirement already satisfied: annotated-types>=0.4.0 in c:\users\444\anaconda3\lib\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)
Requirement already satisfied: pydantic-core==2.20.1 in c:\users\444\anaconda3\lib\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)
Requirement already satisfied: typing-extensions>=4.6.1 in c:\users\444\anaconda3\lib\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\444\anaconda3\lib\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\444\anaconda3\lib\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\444\anaconda3\lib\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\444\anaconda3\lib\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)
Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\users\444\anaconda3\lib\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)
Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\users\444\anaconda3\lib\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)
Requirement already satisfied: colorama in c:\users\444\anaconda3\lib\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)
Requirement already satisfied: click>=8.0.0 in c:\users\444\anaconda3\lib\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)
Requirement already satisfied: shellingham>=1.3.0 in c:\users\444\anaconda3\lib\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)
Requirement already satisfied: rich>=10.11.0 in c:\users\444\anaconda3\lib\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)
Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\users\444\anaconda3\lib\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)
Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\users\444\anaconda3\lib\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)
Requirement already satisfied: MarkupSafe>=2.0 in c:\users\444\anaconda3\lib\site-packages (from jinja2->spacy) (2.1.3)
Requirement already satisfied: marisa-trie>=1.1.0 in c:\users\444\anaconda3\lib\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)
Requirement already satisfied: markdown-it-py>=2.2.0 in c:\users\444\anaconda3\lib\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\users\444\anaconda3\lib\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)
Requirement already satisfied: mdurl~=0.1 in c:\users\444\anaconda3\lib\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)
!python -m spacy download en_core_web_sm
Collecting en-core-web-sm==3.8.0
  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)
     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--
      --------------------------------------- 0.3/12.8 MB ? eta -:--:--
     ------- -------------------------------- 2.4/12.8 MB 10.3 MB/s eta 0:00:02
     ---------------------- ----------------- 7.1/12.8 MB 17.4 MB/s eta 0:00:01
     ---------------------------------- ---- 11.3/12.8 MB 18.0 MB/s eta 0:00:01
     --------------------------------------- 12.8/12.8 MB 16.7 MB/s eta 0:00:00
[+] Download and installation successful
You can now load the package via spacy.load('en_core_web_sm')
!pip install --upgrade numpy
!pip install --force-reinstall --no-cache-dir h5py
Requirement already satisfied: numpy in c:\users\444\anaconda3\lib\site-packages (2.2.6)
Collecting h5py
  Downloading h5py-3.13.0-cp312-cp312-win_amd64.whl.metadata (2.5 kB)
Collecting numpy>=1.19.3 (from h5py)
  Downloading numpy-2.2.6-cp312-cp312-win_amd64.whl.metadata (60 kB)
Downloading h5py-3.13.0-cp312-cp312-win_amd64.whl (3.0 MB)
   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--
   ------- -------------------------------- 0.5/3.0 MB 4.2 MB/s eta 0:00:01
   ---------------------------------------- 3.0/3.0 MB 12.3 MB/s eta 0:00:00
Downloading numpy-2.2.6-cp312-cp312-win_amd64.whl (12.6 MB)
   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--
   ------- -------------------------------- 2.4/12.6 MB 11.2 MB/s eta 0:00:01
   ------------------------------- -------- 10.0/12.6 MB 24.9 MB/s eta 0:00:01
   ---------------------------------------- 12.6/12.6 MB 22.6 MB/s eta 0:00:00
Installing collected packages: numpy, h5py
  Attempting uninstall: numpy
    Found existing installation: numpy 2.2.6
    Uninstalling numpy-2.2.6:
      Successfully uninstalled numpy-2.2.6
  Attempting uninstall: h5py
    Found existing installation: h5py 3.13.0
    Uninstalling h5py-3.13.0:
      Successfully uninstalled h5py-3.13.0
Successfully installed h5py-3.13.0 numpy-2.2.6
  WARNING: Failed to remove contents in a temporary directory 'C:\Users\444\anaconda3\Lib\site-packages\~-mpy.libs'.
  You can safely remove it manually.
  WARNING: Failed to remove contents in a temporary directory 'C:\Users\444\anaconda3\Lib\site-packages\~~mpy'.
  You can safely remove it manually.
  WARNING: Failed to remove contents in a temporary directory 'C:\Users\444\anaconda3\Lib\site-packages\~-py'.
  You can safely remove it manually.
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.2.6 which is incompatible.
gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.
numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.
!python -m spacy download en_core_web_sm
Collecting en-core-web-sm==3.8.0
  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)
[+] Download and installation successful
You can now load the package via spacy.load('en_core_web_sm')
!!python -m spacy validate
['',
 '| Loading compatibility table...',
 '/ Loading compatibility table...',
 '\x1b[2K\x1b[38;5;2m[+] Loaded compatibility table\x1b[0m',
 '\x1b[1m',
 '================= Installed pipeline packages (spaCy v3.8.7) =================\x1b[0m',
 '\x1b[38;5;4m[i] spaCy installation:',
 'C:\\Users\\444\\anaconda3\\Lib\\site-packages\\spacy\x1b[0m',
 '',
 'NAME             SPACY            VERSION                              ',
 'en_core_web_sm   >=3.8.0,<3.9.0   \x1b[38;5;2m3.8.0\x1b[0m   \x1b[38;5;2m[+]\x1b[0m',
 '']
import spacy
nlp = spacy.load("en_core_web_sm")
1. Named Entities Extraction
doc = nlp("Taylor Swift performed in Los Angeles on March 3rd, 2023.")

for entity in doc.ents:
    print(entity,",", entity.label_)
Taylor Swift , PERSON
Los Angeles , GPE
March 3rd, 2023 , DATE
2. Entity Classification
def ent_in_sentance(sentance):
    doc = nlp(sentance)
    for entity in doc.ents:
        if entity.label_ == "PERSON":
            print(entity)

ent_in_sentance("Serena Williams had dinner with Tom Hanks in Paris.")
Serena Williams
Tom Hanks
3. Lemmatization
def lemma_in_sentance(sentance):
    doc = nlp(sentance)
    for token in doc:
        print(token.text, " ----> ", token.lemma_)


lemma_in_sentance("She was running and had run 5 kilometers by 7am.")
She  ---->  she
was  ---->  be
running  ---->  run
and  ---->  and
had  ---->  have
run  ---->  run
5  ---->  5
kilometers  ---->  kilometer
by  ---->  by
7  ---->  7
am  ---->  am
.  ---->  .
4. Stop Word Removal
def print_not_stop_words(sentance):
    doc = nlp(sentance)
    not_stop_words = []
    for token in doc:
        if not token.is_stop and not token.is_punct:
            not_stop_words.append(token.text)
    print(not_stop_words)

print_not_stop_words("This is an example sentence with some stop words.")
['example', 'sentence', 'stop', 'words']
5. Custom Stop Word
doc = nlp("SpaCy is awesome and powerful.")
nlp.vocab["powerful"].is_stop = True

for token in doc:
    if token.is_stop:
        print(token)
is
and
powerful
6. Phrase Matcher
from spacy.matcher import PhraseMatcher
matcher = PhraseMatcher(nlp.vocab)

patterns = [nlp("artificial intelligence"),
            nlp("Artificial Intelligence"),
            nlp("artificial-intelligence"),
            nlp("Artificial-Intelligence"),
            nlp("AI")]

matcher.add("artificial intelligence", patterns)

doc = nlp("Artificial Intelligence is the future. I study artificial intelligence. Sometimes, when I think of AI i wonder how artificial-intelligence will change the world we live in.")
matches = matcher(doc)

for match_id, start, end in matches:
    print(doc[start:end].text)
Artificial Intelligence
artificial intelligence
AI
artificial-intelligence
7. POS Tagging + Explanation
from spacy import explain

doc = nlp("The cat sat on the mat.")
for token in doc:
    print(token.text,", ", token.pos_,", ", spacy.explain(token.pos_))
The ,  DET ,  determiner
cat ,  NOUN ,  noun
sat ,  VERB ,  verb
on ,  ADP ,  adposition
the ,  DET ,  determiner
mat ,  NOUN ,  noun
. ,  PUNCT ,  punctuation
8. Accessing the Pipeline + Custom Sentence Separator
import spacy
from spacy.language import Language

if "parser" in nlp.pipe_names:
    nlp.remove_pipe("parser")
    
if "sentencizer" not in nlp.pipe_names:
    nlp.add_pipe("sentencizer")

@Language.component('set_custom_boundaries')
def set_custom_boundaries(doc):
    for i, token in enumerate(doc[:-1]):
        if token.text == '^':
            token.is_sent_start = False
            doc[i+1].is_sent_start = True
    return doc

# מוסיפים את השלב אחרי sentencizer – כדי לשנות את הסימנים
if "set_custom_boundaries" not in nlp.pipe_names:
    nlp.add_pipe("set_custom_boundaries", after="sentencizer")

# בדיקה
doc = nlp('SpaCy is great ^ It helps with NLP tasks ^ Really useful.')

for sent in doc.sents:
    print(sent.text.replace("^", "").strip())
SpaCy is great
It helps with NLP tasks
Really useful.
9. POS Tagging + Displacy Visualization
from spacy import displacy

sentence = input("Enter a sentence: ")
doc = nlp(sentence)

for token in doc:
    print(token.text,"  ", token.pos_,"  ", spacy.explain(token.pos_))

displacy.render(doc, style="dep", jupyter=True)
Sometimes    ADV    adverb
I    PRON    pronoun
feel    VERB    verb
lost    VERB    verb
within    ADP    adposition
this    DET    determiner
crazy    ADJ    adjective
world    NOUN    noun
of    ADP    adposition
AI    PROPN    proper noun
!    PUNCT    punctuation
Help    VERB    verb
!    PUNCT    punctuation
!    PUNCT    punctuation
!    PUNCT    punctuation
C:\Users\444\anaconda3\Lib\site-packages\spacy\displacy\__init__.py:141: UserWarning: [W005] Doc object not parsed. This means displaCy won't be able to generate a dependency visualization for it. Make sure the Doc was processed with a model that supports dependency parsing, and not just a language class like `English()`. For more info, see the docs:
https://spacy.io/usage/models
  warnings.warn(Warnings.W005)
SometimesADV
IPRON
feelVERB
lostVERB
withinADP
thisDET
crazyADJ
worldNOUN
ofADP
AI!PROPN
Help!!!VERB
print(nlp.pipe_names)
doc = nlp('I love you mom, you are so great it meakes my heart explode and my eyes want to pop out! wow!! ')
print (doc.text)
print()

for token in doc:
    print(token)
    print()
    print("token.text - ", token.text)
    print("token.ent_type_ -", token.ent_type_ if token.ent_type_ else "Not entity")
    print("token.ent_iob_ - ", token.ent_iob_)
    print("token.pos_ - ", token.pos_)
    print("spacy.explain(token.pos_) - ", spacy.explain(token.pos_))
    print("token.dep_ - ", token.dep_)
    print("spacy.explain(token.dep_) - ", spacy.explain(token.dep_))
    print("token.is_stop  - ", token.is_stop )
    print("token.is_punct - ", token.is_punct)
    print("token.like_num - ", token.like_num)
    print("token.is_sent_start - ", token.is_sent_start)
    print("token.lemma_ - ", token.lemma_)
    print("token.tag_ - ", token.tag_)
    print("token.is_alpha - ", token.is_alpha)
    print("token.morph - ", token.morph)
    print("token.idx - ", token.idx)
    print()
    print()
    
print("===== Named Entities (doc.ents) =====")
for ent in doc.ents:
    print(f"{ent.text:<25} {ent.label_:<10} {spacy.explain(ent.label_)}")
I love you mom, you are so great it meakes my heart explode and my eyes want to pop out! wow!! 

I

token.text -  I
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  PRON
spacy.explain(token.pos_) -  pronoun
token.dep_ -  nsubj
spacy.explain(token.dep_) -  nominal subject
token.is_stop  -  True
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  True
token.lemma_ -  I
token.tag_ -  PRP
token.is_alpha -  True
token.morph -  Case=Nom|Number=Sing|Person=1|PronType=Prs
token.idx -  0


love

token.text -  love
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  VERB
spacy.explain(token.pos_) -  verb
token.dep_ -  ccomp
spacy.explain(token.dep_) -  clausal complement
token.is_stop  -  False
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  love
token.tag_ -  VBP
token.is_alpha -  True
token.morph -  Tense=Pres|VerbForm=Fin
token.idx -  2


you

token.text -  you
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  PRON
spacy.explain(token.pos_) -  pronoun
token.dep_ -  dobj
spacy.explain(token.dep_) -  direct object
token.is_stop  -  True
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  you
token.tag_ -  PRP
token.is_alpha -  True
token.morph -  Case=Acc|Person=2|PronType=Prs
token.idx -  7


mom

token.text -  mom
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  NOUN
spacy.explain(token.pos_) -  noun
token.dep_ -  npadvmod
spacy.explain(token.dep_) -  noun phrase as adverbial modifier
token.is_stop  -  False
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  mom
token.tag_ -  NN
token.is_alpha -  True
token.morph -  Number=Sing
token.idx -  11


,

token.text -  ,
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  PUNCT
spacy.explain(token.pos_) -  punctuation
token.dep_ -  punct
spacy.explain(token.dep_) -  punctuation
token.is_stop  -  False
token.is_punct -  True
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  ,
token.tag_ -  ,
token.is_alpha -  False
token.morph -  PunctType=Comm
token.idx -  14


you

token.text -  you
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  PRON
spacy.explain(token.pos_) -  pronoun
token.dep_ -  nsubj
spacy.explain(token.dep_) -  nominal subject
token.is_stop  -  True
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  you
token.tag_ -  PRP
token.is_alpha -  True
token.morph -  Case=Nom|Person=2|PronType=Prs
token.idx -  16


are

token.text -  are
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  AUX
spacy.explain(token.pos_) -  auxiliary
token.dep_ -  ROOT
spacy.explain(token.dep_) -  root
token.is_stop  -  True
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  be
token.tag_ -  VBP
token.is_alpha -  True
token.morph -  Mood=Ind|Tense=Pres|VerbForm=Fin
token.idx -  20


so

token.text -  so
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  ADV
spacy.explain(token.pos_) -  adverb
token.dep_ -  advmod
spacy.explain(token.dep_) -  adverbial modifier
token.is_stop  -  True
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  so
token.tag_ -  RB
token.is_alpha -  True
token.morph -  
token.idx -  24


great

token.text -  great
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  ADJ
spacy.explain(token.pos_) -  adjective
token.dep_ -  acomp
spacy.explain(token.dep_) -  adjectival complement
token.is_stop  -  False
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  great
token.tag_ -  JJ
token.is_alpha -  True
token.morph -  Degree=Pos
token.idx -  27


it

token.text -  it
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  PRON
spacy.explain(token.pos_) -  pronoun
token.dep_ -  nsubj
spacy.explain(token.dep_) -  nominal subject
token.is_stop  -  True
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  it
token.tag_ -  PRP
token.is_alpha -  True
token.morph -  Case=Nom|Gender=Neut|Number=Sing|Person=3|PronType=Prs
token.idx -  33


meakes

token.text -  meakes
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  VERB
spacy.explain(token.pos_) -  verb
token.dep_ -  ccomp
spacy.explain(token.dep_) -  clausal complement
token.is_stop  -  False
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  meake
token.tag_ -  VBZ
token.is_alpha -  True
token.morph -  Number=Sing|Person=3|Tense=Pres|VerbForm=Fin
token.idx -  36


my

token.text -  my
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  PRON
spacy.explain(token.pos_) -  pronoun
token.dep_ -  poss
spacy.explain(token.dep_) -  possession modifier
token.is_stop  -  True
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  my
token.tag_ -  PRP$
token.is_alpha -  True
token.morph -  Number=Sing|Person=1|Poss=Yes|PronType=Prs
token.idx -  43


heart

token.text -  heart
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  NOUN
spacy.explain(token.pos_) -  noun
token.dep_ -  nsubj
spacy.explain(token.dep_) -  nominal subject
token.is_stop  -  False
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  heart
token.tag_ -  NN
token.is_alpha -  True
token.morph -  Number=Sing
token.idx -  46


explode

token.text -  explode
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  VERB
spacy.explain(token.pos_) -  verb
token.dep_ -  ccomp
spacy.explain(token.dep_) -  clausal complement
token.is_stop  -  False
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  explode
token.tag_ -  VB
token.is_alpha -  True
token.morph -  VerbForm=Inf
token.idx -  52


and

token.text -  and
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  CCONJ
spacy.explain(token.pos_) -  coordinating conjunction
token.dep_ -  cc
spacy.explain(token.dep_) -  coordinating conjunction
token.is_stop  -  True
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  and
token.tag_ -  CC
token.is_alpha -  True
token.morph -  ConjType=Cmp
token.idx -  60


my

token.text -  my
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  PRON
spacy.explain(token.pos_) -  pronoun
token.dep_ -  poss
spacy.explain(token.dep_) -  possession modifier
token.is_stop  -  True
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  my
token.tag_ -  PRP$
token.is_alpha -  True
token.morph -  Number=Sing|Person=1|Poss=Yes|PronType=Prs
token.idx -  64


eyes

token.text -  eyes
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  NOUN
spacy.explain(token.pos_) -  noun
token.dep_ -  nsubj
spacy.explain(token.dep_) -  nominal subject
token.is_stop  -  False
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  eye
token.tag_ -  NNS
token.is_alpha -  True
token.morph -  Number=Plur
token.idx -  67


want

token.text -  want
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  VERB
spacy.explain(token.pos_) -  verb
token.dep_ -  conj
spacy.explain(token.dep_) -  conjunct
token.is_stop  -  False
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  want
token.tag_ -  VBP
token.is_alpha -  True
token.morph -  Tense=Pres|VerbForm=Fin
token.idx -  72


to

token.text -  to
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  PART
spacy.explain(token.pos_) -  particle
token.dep_ -  aux
spacy.explain(token.dep_) -  auxiliary
token.is_stop  -  True
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  to
token.tag_ -  TO
token.is_alpha -  True
token.morph -  
token.idx -  77


pop

token.text -  pop
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  VERB
spacy.explain(token.pos_) -  verb
token.dep_ -  xcomp
spacy.explain(token.dep_) -  open clausal complement
token.is_stop  -  False
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  pop
token.tag_ -  VB
token.is_alpha -  True
token.morph -  VerbForm=Inf
token.idx -  80


out

token.text -  out
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  ADP
spacy.explain(token.pos_) -  adposition
token.dep_ -  prt
spacy.explain(token.dep_) -  particle
token.is_stop  -  True
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  out
token.tag_ -  RP
token.is_alpha -  True
token.morph -  
token.idx -  84


!

token.text -  !
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  PUNCT
spacy.explain(token.pos_) -  punctuation
token.dep_ -  punct
spacy.explain(token.dep_) -  punctuation
token.is_stop  -  False
token.is_punct -  True
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  !
token.tag_ -  .
token.is_alpha -  False
token.morph -  PunctType=Peri
token.idx -  87


wow

token.text -  wow
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  INTJ
spacy.explain(token.pos_) -  interjection
token.dep_ -  ROOT
spacy.explain(token.dep_) -  root
token.is_stop  -  False
token.is_punct -  False
token.like_num -  False
token.is_sent_start -  True
token.lemma_ -  wow
token.tag_ -  UH
token.is_alpha -  True
token.morph -  
token.idx -  89


!

token.text -  !
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  PUNCT
spacy.explain(token.pos_) -  punctuation
token.dep_ -  punct
spacy.explain(token.dep_) -  punctuation
token.is_stop  -  False
token.is_punct -  True
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  !
token.tag_ -  .
token.is_alpha -  False
token.morph -  PunctType=Peri
token.idx -  92


!

token.text -  !
token.ent_type_ - Not entity
token.ent_iob_ -  O
token.pos_ -  PUNCT
spacy.explain(token.pos_) -  punctuation
token.dep_ -  punct
spacy.explain(token.dep_) -  punctuation
token.is_stop  -  False
token.is_punct -  True
token.like_num -  False
token.is_sent_start -  False
token.lemma_ -  !
token.tag_ -  .
token.is_alpha -  False
token.morph -  PunctType=Peri
token.idx -  93


===== Named Entities (doc.ents) =====
 
